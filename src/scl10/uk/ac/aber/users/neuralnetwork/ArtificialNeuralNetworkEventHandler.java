package scl10.uk.ac.aber.users.neuralnetwork;

import java.util.ArrayList;

import org.neuroph.core.events.LearningEvent;
import org.neuroph.core.events.LearningEventListener;
import org.neuroph.nnet.learning.BackPropagation;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import lombok.Getter;

/**
 * A class used to handle learning events within an artificial neural network.
 * 
 * @author Scott Lockett
 */
class ArtificialNeuralNetworkEventHandler implements LearningEventListener
{

	/**
	 * Logger for logging user information
	 */
	private static final Logger LOGGER = LoggerFactory.getLogger(ArtificialNeuralNetworkEventHandler.class);

	/**
	 * A list of the network error generated by the back propergation. This is
	 * used for testing. I know I shouldn't really be adding code to the classes
	 * to fit the test but once the train method is called it doesn't pop off
	 * the java program stack until it's been fully trained, so there was no way
	 * of intercepting each iteration to test to see if it was reducing or not.
	 */
	@Getter
	private final ArrayList<Double> networkErrorList = new ArrayList<>();

	/**
	 * Code taken from <a href=
	 * "https://github.com/neuroph/neuroph/blob/master/neuroph-2.9/Samples/src/main/java/org/neuroph/samples/uci/ForestFiresSample.java">
	 * here. </a> Shows the error rate of the artificial neural network. The
	 * error rate should be reducing after each iteration.
	 */
	@Override
	public void handleLearningEvent(final LearningEvent event)
	{
		/*
		 * Get the back propergation object associated with the event.
		 */
		final BackPropagation bp = (BackPropagation) event.getSource();

		/*
		 * get the current network error
		 */
		final Double currentNetworkError = bp.getTotalNetworkError();

		/*
		 * Add it to network error list
		 */
		networkErrorList.add(currentNetworkError);

		/*
		 * Print the current network error
		 */
		LOGGER.info(bp.getCurrentIteration() + ". iteration | Total network error: " + currentNetworkError);
	}

}
